"""
Media Downloader - Python Companion
Downloads both images and videos based on unified JSON index files generated by the Tampermonkey script.
Supports unified_media_index_*.json (v3.0+ - mixed videos and images)

USAGE:
    python media_downloader.py path/to/unified_media_index_20250812.json
    python media_downloader.py --type videos unified_media_index.json
    python media_downloader.py --type images unified_media_index.json
    python media_downloader.py --concurrent 10 --delay 0.1 index.json

DEFAULT FOLDERS:
    - Videos: Downloads to ../videos/ (parent directory)
    - Images: Downloads to ../images/ (parent directory)
    - Custom: Use --folder to override default location
    
FEATURES:
    - Supports both video and image downloading
    - Async/parallel downloads for speed
    - Content-based deduplication using MD5 hashes
    - Automatic retry on failures
    - Progress tracking and detailed statistics
    - Preserves metadata in JSON files
    - Respects cookies and headers from browser
    
REQUIREMENTS:
    pip install aiohttp aiofiles
    
INDEX FILES:
    Generate index files using the companion Tampermonkey script on imgur.com:
    - Videos: Click "Scan Current Page" then "Download Index JSON" 
    - Images: Click "Scan Images" then "Download Images JSON"
"""

import os
import json
import sys
import asyncio
import aiohttp
import aiofiles
import time
import argparse
from pathlib import Path
from datetime import datetime
from urllib.parse import urlparse, unquote
import urllib3
import hashlib
import uuid

# Disable SSL warnings for problematic sites
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class MediaDownloader:
    def __init__(self, index_file, download_folder=None, media_type_filter='both', max_concurrent=5):
        self.index_file = Path(index_file)
        self.index_data = {}
        self.download_folder = Path.cwd()
        self.max_concurrent = max_concurrent
        self.session = None
        self.media_type_filter = media_type_filter  # 'both', 'videos', 'images'
        
        # Fast content-hash algorithm for deduplication
        self.hash_algo = 'md5'
        self.stats = {
            'total': 0,
            'downloaded': 0,
            'skipped': 0,
            'skipped_by_hash': 0,
            'failed': 0,
            'errors': [],
            'in_progress': 0
        }
        # Hash index for deduplication
        self.hash_index = {
            'algo': self.hash_algo,
            'files': {}  # hash -> {'path': 'filename', 'size': int}
        }
        self.hash_index_file = None

        self.load_index()
        if download_folder:
            self.download_folder = Path(download_folder)
        else:
            # Always use separate folders for videos and images
            self.setup_download_folders()
        
        # Prepare hash index path and load/build
        # For mixed downloads, put hash index in parent directory
        if self.media_type_filter == 'both':
            parent_dir = Path(__file__).parent.parent  
            self.hash_index_file = parent_dir / ".hash_index.json"
        else:
            self.hash_index_file = self.download_folder / ".hash_index.json"
        self._load_or_build_hash_index()

    def load_index(self):
        try:
            with open(self.index_file, 'r', encoding='utf-8') as f:
                self.index_data = json.load(f)
            
            # Only support unified media index format
            if 'media' not in self.index_data:
                raise ValueError("Index file must contain 'media' array. Only unified media index files are supported.")
                
            media_items = self.index_data['media']
            
            # Filter media items by type if specified
            if self.media_type_filter == 'videos':
                media_items = [item for item in media_items if item.get('mediaType') == 'video']
            elif self.media_type_filter == 'images':
                media_items = [item for item in media_items if item.get('mediaType') == 'image']
            
            # Store filtered items back
            self.filtered_media = media_items
            
            # Count media types for display
            videos = sum(1 for item in media_items if item.get('mediaType') == 'video')
            images = sum(1 for item in media_items if item.get('mediaType') == 'image')
            
            if self.media_type_filter == 'both':
                print(f"‚úÖ Loaded unified media index: {videos} videos, {images} images ({len(media_items)} total)")
            elif self.media_type_filter == 'videos':
                print(f"‚úÖ Loaded unified media index (videos only): {videos} videos")
            else:
                print(f"‚úÖ Loaded unified media index (images only): {images} images")
                
            self.stats['total'] = len(media_items)
            print(f"üìÅ Generated: {self.index_data.get('generated', 'unknown')}")
            print(f"üåê Page: {self.index_data.get('pageUrl', 'unknown')}")
        except Exception as e:
            print(f"‚ùå Error loading index file: {e}")
            sys.exit(1)
    
    def setup_download_folders(self):
        """Set up separate video and image download folders"""
        script_dir = Path(__file__).parent
        parent_dir = script_dir.parent
        
        # Define individual folders
        self.videos_folder = parent_dir / 'videos'
        self.images_folder = parent_dir / 'images'
        
        if self.media_type_filter == 'videos':
            self.download_folder = self.videos_folder
        elif self.media_type_filter == 'images':
            self.download_folder = self.images_folder
        else:  # both types
            # For mixed downloads, we'll dynamically choose folder per item
            # Set a default for hash index location
            self.download_folder = parent_dir / 'media'  # temp, will be overridden per item
        
        # Create all folders that might be needed
        if self.media_type_filter in ['both', 'videos']:
            self.videos_folder.mkdir(parents=True, exist_ok=True)
        if self.media_type_filter in ['both', 'images']:
            self.images_folder.mkdir(parents=True, exist_ok=True)
        self.download_folder.mkdir(parents=True, exist_ok=True)

    async def create_session(self):
        config = self.index_data.get('downloadConfig', {})
        headers = {
            'User-Agent': config.get('userAgent', 'Mozilla/5.0'),
            'Referer': config.get('referer', 'https://example.com/'),
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        # Set appropriate Accept header - use generic for mixed content
        headers['Accept'] = config.get('headers', {}).get('Accept', '*/*')
        
        if 'headers' in config:
            headers.update(config['headers'])
        connector = aiohttp.TCPConnector(ssl=False, limit=50, limit_per_host=10)
        timeout = aiohttp.ClientTimeout(total=60, connect=30)
        self.session = aiohttp.ClientSession(headers=headers, connector=connector, timeout=timeout)
        cookies_str = config.get('cookies', '')
        if cookies_str:
            cookies_dict = {}
            for cookie in cookies_str.split(';'):
                if '=' in cookie:
                    name, value = cookie.strip().split('=', 1)
                    cookies_dict[name] = value
            if cookies_dict:
                for name, value in cookies_dict.items():
                    self.session.cookie_jar.update_cookies({name: value})
        print(f"üîß Async session created with headers and cookies")

    def sanitize_filename(self, filename):
        if not filename:
            return "unknown.jpg"
        invalid = '<>:"|?*\\/\0'
        for ch in invalid:
            filename = filename.replace(ch, '_')
        if len(filename) > 200:
            name, ext = os.path.splitext(filename)
            filename = name[:190] + ext
        return filename

    def _list_existing_media_files(self):
        # Support extensions for both images and videos
        image_exts = {'.jpg', '.jpeg', '.png', '.gif', '.webp', '.avif', '.bmp', '.tif', '.tiff'}
        video_exts = {'.mp4', '.webm', '.avi', '.mkv', '.mov', '.wmv', '.flv', '.m4v'}
        
        # Always check both types since we support unified media now
        valid_exts = image_exts | video_exts
            
        hi_name = self.hash_index_file.name if self.hash_index_file else None
        
        # For mixed downloads, scan both folders
        if self.media_type_filter == 'both':
            folders_to_scan = [self.videos_folder, self.images_folder]
        else:
            folders_to_scan = [self.download_folder]
            
        for folder in folders_to_scan:
            if not folder.exists():
                continue
            for p in folder.iterdir():
                if not p.is_file():
                    continue
                if hi_name and p.name == hi_name:
                    continue
                if p.name.endswith('_metadata.json'):
                    continue
                if p.suffix.lower() in valid_exts:
                    yield p

    def _new_hasher(self):
        if self.hash_algo.lower() == 'md5':
            return hashlib.md5()
        if self.hash_algo.lower() == 'blake2b':
            return hashlib.blake2b(digest_size=16)
        # Fallback
        return hashlib.sha256()

    def _compute_hash(self, path: Path, chunk_size: int = 1024 * 1024) -> str:
        h = self._new_hasher()
        with open(path, 'rb') as f:
            while True:
                buf = f.read(chunk_size)
                if not buf:
                    break
                h.update(buf)
        return h.hexdigest()

    def _load_or_build_hash_index(self):
        if not self.hash_index_file:
            self.hash_index_file = self.download_folder / ".hash_index.json"
        try:
            if self.hash_index_file.exists():
                with open(self.hash_index_file, 'r', encoding='utf-8') as f:
                    loaded = json.load(f)
                if isinstance(loaded, dict) and 'files' in loaded:
                    self.hash_index = loaded
                    # Always use our configured algo (md5) as requested
                    self.hash_index['algo'] = self.hash_algo
                    print(f"üîé Loaded existing hash index with {len(self.hash_index['files'])} entries")
                    return
        except Exception as e:
            print(f"‚ö†Ô∏è  Could not load hash index ({e}), will attempt to rebuild")

        # Build from existing files
        print(f"üßÆ Building hash index for existing media files (one-time scan)...")
        count = 0
        for p in self._list_existing_media_files():
            try:
                sha = self._compute_hash(p)
                rel = p.name
                self.hash_index['files'][sha] = {'path': rel, 'size': p.stat().st_size}
                count += 1
                if count % 20 == 0:
                    print(f"   ‚Ä¢ Hashed {count} files...")
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Failed to hash {p.name}: {e}")
        self._save_hash_index()
        print(f"‚úÖ Hash index ready with {len(self.hash_index['files'])} entries")

    def _save_hash_index(self):
        try:
            if not self.hash_index_file:
                self.hash_index_file = self.download_folder / ".hash_index.json"
            # Ensure the index reflects current algorithm choice
            self.hash_index['algo'] = self.hash_algo
            tmp = self.hash_index_file.with_suffix(self.hash_index_file.suffix + ".tmp")
            with open(tmp, 'w', encoding='utf-8') as f:
                json.dump(self.hash_index, f, indent=2)
            os.replace(tmp, self.hash_index_file)
        except Exception as e:
            print(f"‚ö†Ô∏è  Failed to save hash index: {e}")

    def _ensure_unique_path(self, target: Path) -> Path:
        if not target.exists():
            return target
        stem, ext = target.stem, target.suffix
        for i in range(1, 1000):
            candidate = target.with_name(f"{stem} ({i}){ext}")
            if not candidate.exists():
                return candidate
        # As a last resort, append a uuid
        return target.with_name(f"{stem} ({uuid.uuid4().hex}){ext}")

    async def download_media(self, media_data, media_index, semaphore):
        async with semaphore:
            # Get media type and URL from unified format
            item_media_type = media_data.get('mediaType', 'unknown')
            if item_media_type == 'video':
                url = media_data.get('videoUrl')
                media_type_name = 'video'
            elif item_media_type == 'image':
                url = media_data.get('imageUrl') or media_data.get('thumbUrl')
                media_type_name = 'image'
            else:
                print(f"‚ùå [{media_index + 1}/{self.stats['total']}] Unknown media type: {item_media_type}")
                self.stats['failed'] += 1
                return False
                
            # Check if we should filter this media type
            if self.media_type_filter != 'both':
                filter_type = 'videos' if item_media_type == 'video' else 'images'
                if filter_type != self.media_type_filter:
                    # Skip this item silently
                    return True
                
            if not url:
                print(f"‚ùå [{media_index + 1}/{self.stats['total']}] No {media_type_name} URL")
                self.stats['failed'] += 1
                return False
                
            filename = media_data.get('filename')
            if not filename:
                parsed = urlparse(url)
                filename = Path(parsed.path).name or f"{media_type_name}_{media_index+1}.{'mp4' if item_media_type == 'video' else 'jpg'}"
            filename = self.sanitize_filename(filename)
            
            # Choose the right folder based on media type
            if item_media_type == 'video':
                target_folder = self.videos_folder
            else:  # image
                target_folder = self.images_folder
            
            file_path = target_folder / filename
            metadata_path = file_path.with_suffix(file_path.suffix + '_metadata.json')
            
            try:
                self.stats['in_progress'] += 1
                print(f"‚¨áÔ∏è  [{media_index + 1}/{self.stats['total']}] Starting {media_type_name}: {filename} (Active: {self.stats['in_progress']})")
                s = self.session
                if s is None:
                    raise RuntimeError("HTTP session not initialized")
                    
                async with s.get(url) as response:
                    response.raise_for_status()
                    content_type = response.headers.get('content-type', '')
                    
                    # Validate content type based on item type
                    expected_type = 'video/' if item_media_type == 'video' else 'image/'
                    if not content_type.startswith(expected_type):
                        print(f"‚ö†Ô∏è  [{media_index + 1}] Unexpected content type: {content_type} (expected {expected_type}*)")
                    
                    # Download to temp while computing hash
                    temp_path = file_path.with_suffix(file_path.suffix + f".part.{uuid.uuid4().hex}")
                    hasher = self._new_hasher()
                    try:
                        async with aiofiles.open(temp_path, 'wb') as f:
                            async for chunk in response.content.iter_chunked(1024 * 64):
                                if not chunk:
                                    continue
                                await f.write(chunk)
                                hasher.update(chunk)
                        sha = hasher.hexdigest()
                        
                        # Check for duplicates by content
                        if sha in self.hash_index['files']:
                            # Duplicate content found; do not keep the temp file
                            try:
                                os.remove(temp_path)
                            except Exception:
                                pass
                            existing = self.hash_index['files'][sha]['path']
                            self.stats['skipped_by_hash'] += 1
                            self.stats['in_progress'] -= 1
                            print(f"‚è≠Ô∏è  [{media_index + 1}/{self.stats['total']}] Duplicate by hash; existing: {existing}")
                            return True
                            
                        # Move to a unique target name to avoid overwrites
                        final_path = self._ensure_unique_path(file_path)
                        os.replace(temp_path, final_path)
                        
                        # Write metadata
                        async with aiofiles.open(final_path.with_suffix(final_path.suffix + '_metadata.json'), 'w', encoding='utf-8') as f:
                            await f.write(json.dumps(media_data, indent=2, ensure_ascii=False))
                        
                        # Update hash index and stats
                        self.hash_index['files'][sha] = {'path': final_path.name, 'size': final_path.stat().st_size}
                        self._save_hash_index()
                        self.stats['in_progress'] -= 1
                        print(f"‚úÖ [{media_index + 1}/{self.stats['total']}] Downloaded {media_type_name}: {final_path.name} ({final_path.stat().st_size // 1024} KB)")
                        self.stats['downloaded'] += 1
                        return True
                    finally:
                        # Ensure temp file is cleaned up if something went wrong mid-way
                        if temp_path.exists():
                            try:
                                os.remove(temp_path)
                            except Exception:
                                pass
            except aiohttp.ClientError as e:
                err = f"Network error downloading {filename}: {e}"
                print(f"‚ùå [{media_index + 1}/{self.stats['total']}] {err}")
                self.stats['errors'].append(err)
                self.stats['failed'] += 1
                self.stats['in_progress'] -= 1
                return False
            except Exception as e:
                err = f"Error downloading {filename}: {e}"
                print(f"‚ùå [{media_index + 1}/{self.stats['total']}] {err}")
                self.stats['errors'].append(err)
                self.stats['failed'] += 1
                self.stats['in_progress'] -= 1
                return False

    async def download_all_async(self, delay=0.1, max_retries=3):
        # Display filter type
        if self.media_type_filter == 'both':
            filter_display = "all media"
        else:
            filter_display = self.media_type_filter
        
        print(f"\nüöÄ Starting parallel {filter_display} download")
        print(f"üìÅ Images to: {self.images_folder}")
        print(f"üìÅ Videos to: {self.videos_folder}")
        print(f"üîÄ Max concurrent downloads: {self.max_concurrent}")
        print(f"‚è±Ô∏è  Delay between batches: {delay}s")
        print("=" * 60)
        await self.create_session()
        start = time.time()
        semaphore = asyncio.Semaphore(self.max_concurrent)
        tasks = []
        
        # Get filtered media items
        media_items = self.filtered_media
        
        for i, data in enumerate(media_items):
            tasks.append(self.download_with_retry(data, i, semaphore, max_retries, delay))
        await asyncio.gather(*tasks, return_exceptions=True)
        if self.session is not None:
            await self.session.close()
        elapsed = time.time() - start
        self.print_summary(elapsed)

    async def download_with_retry(self, data, idx, semaphore, max_retries, delay):
        success = False
        retries = 0
        while not success and retries < max_retries:
            if retries > 0:
                print(f"üîÑ [{idx + 1}] Retry {retries}/{max_retries}")
                await asyncio.sleep(delay * 2)
            try:
                success = await self.download_media(data, idx, semaphore)
                if not success and retries < max_retries - 1:
                    await asyncio.sleep(delay)
            except Exception as e:
                print(f"‚ùå [{idx + 1}] Retry failed: {e}")
                if retries < max_retries - 1:
                    await asyncio.sleep(delay)
            retries += 1
        return success

    def download_all(self, delay=0.1, max_retries=3):
        try:
            asyncio.run(self.download_all_async(delay, max_retries))
        except KeyboardInterrupt:
            print("\n‚èπÔ∏è  Download interrupted by user")
            raise

    def print_summary(self, elapsed):
        # Display based on filter type
        if self.media_type_filter == 'both':
            media_type_display = "UNIFIED MEDIA"
        else:
            media_type_display = self.media_type_filter.upper()
            
        print("\n" + "=" * 60)
        print(f"üìä {media_type_display} DOWNLOAD SUMMARY")
        print("=" * 60)
        print(f"‚è±Ô∏è  Total time: {elapsed:.1f}s")
        if self.media_type_filter == 'both':
            print(f"üìÅ Videos folder: {self.videos_folder}")
            print(f"üìÅ Images folder: {self.images_folder}")
        else:
            print(f"üìÅ Download folder: {self.download_folder}")
        print(f"üé¨ Total items: {self.stats['total']}")
        print(f"‚úÖ Downloaded: {self.stats['downloaded']}")
        print(f"‚è≠Ô∏è  Skipped (existing): {self.stats['skipped']}")
        print(f"‚è≠Ô∏è  Skipped (duplicate by hash): {self.stats['skipped_by_hash']}")
        print(f"‚ùå Failed: {self.stats['failed']}")
        if self.stats['errors']:
            print(f"\n‚ö†Ô∏è  ERRORS ({len(self.stats['errors'])}):")
            for e in self.stats['errors'][:10]:
                print(f"   ‚Ä¢ {e}")
            if len(self.stats['errors']) > 10:
                print(f"   ... and {len(self.stats['errors']) - 10} more")
        success_rate = (self.stats['downloaded'] / max(self.stats['total'], 1)) * 100
        print(f"\nüéØ Success rate: {success_rate:.1f}%")


def main():
    parser = argparse.ArgumentParser(description='Download media files from unified index generated by Tampermonkey script')
    parser.add_argument('index_file', nargs='?', help='Path to JSON index file (unified_media_index_*.json)')
    parser.add_argument('-f', '--folder', help='Download folder (default: ../images/ and ../videos/ for mixed, ../videos/ for videos only, ../images/ for images only)')
    parser.add_argument('-t', '--type', choices=['both', 'videos', 'images'], default='both', 
                       help='Media type to download (default: both). Videos go to ../videos/, images to ../images/')
    parser.add_argument('-c', '--concurrent', type=int, default=5, help='Max concurrent downloads (default: 5)')
    parser.add_argument('-d', '--delay', type=float, default=0.05, help='Delay between batches in seconds (default: 0.05)')
    parser.add_argument('-r', '--retries', type=int, default=5, help='Max retries per file (default: 5)')
    
    args = parser.parse_args()
    
    # If no index file provided, try to find one
    if not args.index_file:
        search_patterns = ['unified_media_index_*.json']
        found_files = []
        for pattern in search_patterns:
            found_files.extend(Path.cwd().glob(pattern))
        
        if not found_files:
            print("‚ùå No index file specified and none found in current directory!")
            print("Usage examples:")
            print("  python media_downloader.py unified_media_index_20250812.json")
            print("  python media_downloader.py unified_media_index_20250812.json --type videos")
            print("  python media_downloader.py unified_media_index_20250812.json --type images")
            print("  python media_downloader.py -h  # for more options")
            sys.exit(1)
        
        # Use most recent file
        args.index_file = max(found_files, key=lambda p: p.stat().st_mtime)
        print(f"üí° Using most recent index file: {args.index_file}")

    INDEX_FILE = str(args.index_file)
    DOWNLOAD_FOLDER = args.folder
    MEDIA_TYPE_FILTER = args.type
    MAX_CONCURRENT = args.concurrent
    DELAY = args.delay
    MAX_RETRIES = args.retries

    print("üé¨ Media Downloader - Unified Index Edition")
    print("Downloads videos and images from unified media index files")
    print("=" * 60)
    print(f"üìÅ Index file: {INDEX_FILE}")
    print(f"üéØ Media type filter: {MEDIA_TYPE_FILTER}")
    print(f"üîÄ Max concurrent: {MAX_CONCURRENT}")
    print(f"‚è±Ô∏è  Delay: {DELAY}s")
    print(f"üîÑ Max retries: {MAX_RETRIES}")

    if not Path(INDEX_FILE).exists():
        print(f"‚ùå Error: Index file '{INDEX_FILE}' not found!")
        print(f"üí° Generate a unified index using the Tampermonkey script, then provide the correct path.")
        print(f"   Expected format: unified_media_index_*.json (v3.0+)")
        sys.exit(1)

    try:
        import aiohttp  # noqa
        import aiofiles  # noqa
    except ImportError:
        print("‚ùå Missing required packages! Install with: pip install aiohttp aiofiles")
        sys.exit(1)

    try:
        d = MediaDownloader(INDEX_FILE, DOWNLOAD_FOLDER, MEDIA_TYPE_FILTER, MAX_CONCURRENT)
        d.download_all(delay=DELAY, max_retries=MAX_RETRIES)
    except KeyboardInterrupt:
        print("\n\n‚èπÔ∏è  Download interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå Fatal error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
